{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "807c1015",
   "metadata": {},
   "source": [
    "## Step 1: Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a086867e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.datasets import load_breast_cancer, load_diabetes, make_classification\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"All libraries imported successfully!\")\n",
    "print(f\"XGBoost version: {xgb.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5add03ac",
   "metadata": {},
   "source": [
    "## Step 2: Load and Prepare Dataset\n",
    "\n",
    "We'll use the Breast Cancer dataset for classification demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee010a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Breast Cancer dataset\n",
    "cancer = load_breast_cancer()\n",
    "X = cancer.data\n",
    "y = cancer.target\n",
    "\n",
    "# Display dataset information\n",
    "print(\"Dataset Information:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Number of samples: {X.shape[0]}\")\n",
    "print(f\"Number of features: {X.shape[1]}\")\n",
    "print(f\"Feature names (first 5): {cancer.feature_names[:5]}\")\n",
    "print(f\"Target names: {cancer.target_names}\")\n",
    "print(f\"Class distribution:\")\n",
    "print(f\"  - Malignant (0): {np.sum(y == 0)}\")\n",
    "print(f\"  - Benign (1): {np.sum(y == 1)}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining set size: {X_train.shape}\")\n",
    "print(f\"Testing set size: {X_test.shape}\")\n",
    "print(f\"Training class distribution: {np.bincount(y_train)}\")\n",
    "print(f\"Testing class distribution: {np.bincount(y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3e31d3",
   "metadata": {},
   "source": [
    "## Step 3: Train Basic XGBoost Classifier\n",
    "\n",
    "Let's start with default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb295fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create XGBoost classifier with basic parameters\n",
    "xgb_clf = XGBClassifier(\n",
    "    n_estimators=100,          # Number of boosting rounds\n",
    "    max_depth=3,               # Maximum depth of trees\n",
    "    learning_rate=0.1,         # Step size shrinkage (eta)\n",
    "    random_state=42,\n",
    "    eval_metric='logloss'      # Evaluation metric\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "xgb_clf.fit(X_train, y_train)\n",
    "\n",
    "print(\"XGBoost Classifier trained successfully!\")\n",
    "print(f\"Number of estimators: {xgb_clf.n_estimators}\")\n",
    "print(f\"Max depth: {xgb_clf.max_depth}\")\n",
    "print(f\"Learning rate: {xgb_clf.learning_rate}\")\n",
    "print(f\"Number of features: {xgb_clf.n_features_in_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2df769",
   "metadata": {},
   "source": [
    "## Step 4: Make Predictions and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672c56bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_train_pred = xgb_clf.predict(X_train)\n",
    "y_test_pred = xgb_clf.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "print(\"XGBoost Performance:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Testing Accuracy:  {test_accuracy:.4f}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nClassification Report (Test Set):\")\n",
    "print(\"=\"*60)\n",
    "print(classification_report(y_test, y_test_pred, target_names=cancer.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58406d7b",
   "metadata": {},
   "source": [
    "## Step 5: Feature Importance Analysis\n",
    "\n",
    "XGBoost provides feature importance scores to understand which features are most influential."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c0321d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance\n",
    "feature_importance = xgb_clf.feature_importances_\n",
    "\n",
    "# Create a dataframe for better visualization\n",
    "feature_df = pd.DataFrame({\n",
    "    'Feature': cancer.feature_names,\n",
    "    'Importance': feature_importance\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"Top 10 Most Important Features:\")\n",
    "print(\"=\"*60)\n",
    "print(feature_df.head(10).to_string(index=False))\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(feature_df['Feature'][:15], feature_df['Importance'][:15])\n",
    "plt.xlabel('Importance Score')\n",
    "plt.ylabel('Features')\n",
    "plt.title('Top 15 Feature Importance (XGBoost)')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Built-in XGBoost plot\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "xgb.plot_importance(xgb_clf, max_num_features=15, ax=ax, importance_type='weight')\n",
    "plt.title('Feature Importance (XGBoost Built-in Plot)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25e4aba",
   "metadata": {},
   "source": [
    "## Step 6: Confusion Matrix Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96492af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create confusion matrix\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=cancer.target_names,\n",
    "            yticklabels=cancer.target_names)\n",
    "plt.title(f'XGBoost Confusion Matrix\\nAccuracy: {test_accuracy:.4f}')\n",
    "plt.ylabel('Actual Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b400d5",
   "metadata": {},
   "source": [
    "## Step 7: Effect of Number of Estimators (n_estimators)\n",
    "\n",
    "Let's see how the number of trees affects performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf748b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different numbers of estimators\n",
    "n_estimators_range = [10, 25, 50, 100, 150, 200, 300, 500]\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "\n",
    "for n_est in n_estimators_range:\n",
    "    xgb_temp = XGBClassifier(\n",
    "        n_estimators=n_est,\n",
    "        max_depth=3,\n",
    "        learning_rate=0.1,\n",
    "        random_state=42,\n",
    "        eval_metric='logloss'\n",
    "    )\n",
    "    xgb_temp.fit(X_train, y_train)\n",
    "    \n",
    "    train_scores.append(xgb_temp.score(X_train, y_train))\n",
    "    test_scores.append(xgb_temp.score(X_test, y_test))\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(n_estimators_range, train_scores, marker='o', label='Training Accuracy', linewidth=2)\n",
    "plt.plot(n_estimators_range, test_scores, marker='s', label='Testing Accuracy', linewidth=2)\n",
    "plt.xlabel('Number of Estimators (Trees)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('XGBoost Performance vs Number of Estimators')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Find optimal n_estimators\n",
    "optimal_n = n_estimators_range[np.argmax(test_scores)]\n",
    "print(f\"Optimal number of estimators: {optimal_n}\")\n",
    "print(f\"Best test accuracy: {max(test_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52027e09",
   "metadata": {},
   "source": [
    "## Step 8: Effect of Learning Rate\n",
    "\n",
    "Learning rate controls how much each tree contributes to the final prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f0c577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different learning rates\n",
    "learning_rates = [0.01, 0.05, 0.1, 0.2, 0.3, 0.5]\n",
    "train_scores_lr = []\n",
    "test_scores_lr = []\n",
    "\n",
    "for lr in learning_rates:\n",
    "    xgb_temp = XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=3,\n",
    "        learning_rate=lr,\n",
    "        random_state=42,\n",
    "        eval_metric='logloss'\n",
    "    )\n",
    "    xgb_temp.fit(X_train, y_train)\n",
    "    \n",
    "    train_scores_lr.append(xgb_temp.score(X_train, y_train))\n",
    "    test_scores_lr.append(xgb_temp.score(X_test, y_test))\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(learning_rates, train_scores_lr, marker='o', label='Training Accuracy', linewidth=2)\n",
    "plt.plot(learning_rates, test_scores_lr, marker='s', label='Testing Accuracy', linewidth=2)\n",
    "plt.xlabel('Learning Rate')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('XGBoost Performance vs Learning Rate')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"Learning Rate Analysis:\")\n",
    "print(\"-\"*60)\n",
    "print(\"Small learning rate (e.g., 0.01):\")\n",
    "print(\"  - More conservative updates\")\n",
    "print(\"  - Requires more trees (higher n_estimators)\")\n",
    "print(\"  - More robust, less prone to overfitting\")\n",
    "print(\"\\nLarge learning rate (e.g., 0.3-0.5):\")\n",
    "print(\"  - Aggressive updates\")\n",
    "print(\"  - Faster training\")\n",
    "print(\"  - Risk of overfitting\")\n",
    "print(\"-\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e60165",
   "metadata": {},
   "source": [
    "## Step 9: Effect of Max Depth\n",
    "\n",
    "Max depth controls the complexity of individual trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ca80d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different max depths\n",
    "max_depths = [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "train_scores_depth = []\n",
    "test_scores_depth = []\n",
    "\n",
    "for depth in max_depths:\n",
    "    xgb_temp = XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=depth,\n",
    "        learning_rate=0.1,\n",
    "        random_state=42,\n",
    "        eval_metric='logloss'\n",
    "    )\n",
    "    xgb_temp.fit(X_train, y_train)\n",
    "    \n",
    "    train_scores_depth.append(xgb_temp.score(X_train, y_train))\n",
    "    test_scores_depth.append(xgb_temp.score(X_test, y_test))\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(max_depths, train_scores_depth, marker='o', label='Training Accuracy', linewidth=2)\n",
    "plt.plot(max_depths, test_scores_depth, marker='s', label='Testing Accuracy', linewidth=2)\n",
    "plt.xlabel('Max Depth')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('XGBoost Performance vs Max Depth')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(max_depths)\n",
    "plt.show()\n",
    "\n",
    "# Find optimal max_depth\n",
    "optimal_depth = max_depths[np.argmax(test_scores_depth)]\n",
    "print(f\"Optimal max depth: {optimal_depth}\")\n",
    "print(f\"Best test accuracy: {max(test_scores_depth):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0766235",
   "metadata": {},
   "source": [
    "## Key Parameters and Best Practices\n",
    "\n",
    "### Key Parameters in XGBoost:\n",
    "\n",
    "**Tree-Specific Parameters:**\n",
    "- **n_estimators**: Number of boosting rounds (trees)\n",
    "  - More trees = better performance but longer training\n",
    "  - Typical range: 100-1000\n",
    "\n",
    "- **max_depth**: Maximum depth of trees\n",
    "  - Controls tree complexity\n",
    "  - Typical range: 3-10\n",
    "  - Deeper = more complex, risk of overfitting\n",
    "\n",
    "- **learning_rate (eta)**: Step size shrinkage\n",
    "  - Lower = more robust but needs more trees\n",
    "  - Typical range: 0.01-0.3\n",
    "\n",
    "- **subsample**: Fraction of samples for each tree\n",
    "  - Prevents overfitting\n",
    "  - Typical range: 0.5-1.0\n",
    "\n",
    "- **colsample_bytree**: Fraction of features for each tree\n",
    "  - Random feature selection per tree\n",
    "  - Typical range: 0.5-1.0\n",
    "\n",
    "**Regularization Parameters:**\n",
    "- **gamma**: Minimum loss reduction for split\n",
    "  - Higher = more conservative\n",
    "  - Range: 0-infinity\n",
    "\n",
    "- **reg_alpha**: L1 regularization (Lasso)\n",
    "  - Feature selection\n",
    "  - Default: 0\n",
    "\n",
    "- **reg_lambda**: L2 regularization (Ridge)\n",
    "  - Smooth weights\n",
    "  - Default: 1\n",
    "\n",
    "### Best Practices:\n",
    "\n",
    "1. **Start Simple**: Begin with default parameters, then tune\n",
    "\n",
    "2. **Learning Rate Trade-off**:\n",
    "   - Low learning rate + many estimators = better performance\n",
    "   - High learning rate + fewer estimators = faster training\n",
    "\n",
    "3. **Use Early Stopping**: \n",
    "   - Monitor validation set\n",
    "   - Stop when no improvement\n",
    "\n",
    "4. **Handle Imbalanced Data**: \n",
    "   - Use `scale_pos_weight` parameter\n",
    "   - Ratio of negative to positive samples\n",
    "\n",
    "5. **Regularization**:\n",
    "   - Increase `gamma`, `reg_alpha`, `reg_lambda` to reduce overfitting\n",
    "   - Decrease `max_depth`, increase `min_child_weight`\n",
    "\n",
    "6. **Feature Engineering**:\n",
    "   - XGBoost benefits from good features\n",
    "   - Remove highly correlated features\n",
    "\n",
    "7. **Cross-Validation**: Always validate with CV before final model\n",
    "\n",
    "### Typical Parameter Combinations:\n",
    "\n",
    "**Fast Training (Quick Baseline):**\n",
    "```python\n",
    "n_estimators=100, max_depth=3, learning_rate=0.1\n",
    "```\n",
    "\n",
    "**High Accuracy (Competition):**\n",
    "```python\n",
    "n_estimators=1000, max_depth=5, learning_rate=0.01,\n",
    "subsample=0.8, colsample_bytree=0.8\n",
    "```\n",
    "\n",
    "**Prevent Overfitting:**\n",
    "```python\n",
    "max_depth=3, min_child_weight=5, gamma=1,\n",
    "subsample=0.7, colsample_bytree=0.7\n",
    "```\n",
    "\n",
    "## Conclusion:\n",
    "\n",
    "XGBoost is a state-of-the-art gradient boosting algorithm that:\n",
    "- Provides exceptional performance and accuracy\n",
    "- Uses advanced regularization to prevent overfitting\n",
    "- Efficiently handles large datasets with parallel processing\n",
    "- Automatically manages missing values\n",
    "- Offers comprehensive feature importance analysis\n",
    "- Dominates machine learning competitions\n",
    "\n",
    "**Key Insight**: XGBoost's success comes from combining gradient boosting with regularization, efficient computation, and smart handling of edge cases. The key to mastering XGBoost is understanding the balance between model complexity (max_depth, n_estimators) and regularization (gamma, lambda, alpha) to achieve optimal performance!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
