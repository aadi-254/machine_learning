{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "241b048e",
   "metadata": {},
   "source": [
    "# AdaBoost (Adaptive Boosting)\n",
    "\n",
    "## What is AdaBoost?\n",
    "\n",
    "**AdaBoost** (Adaptive Boosting) is a powerful ensemble learning algorithm that combines multiple weak classifiers to create a strong classifier. It was introduced by Yoav Freund and Robert Schapire in 1996.\n",
    "\n",
    "## Key Concepts:\n",
    "\n",
    "1. **Weak Learners**: Simple models that perform slightly better than random guessing (typically decision stumps - decision trees with depth 1)\n",
    "\n",
    "2. **Sequential Learning**: Models are trained sequentially, where each new model focuses on correcting the errors of previous models\n",
    "\n",
    "3. **Weight Adjustment**: AdaBoost adjusts the weights of incorrectly classified instances so that subsequent models focus more on difficult cases\n",
    "\n",
    "4. **Final Prediction**: Weighted vote of all weak learners\n",
    "\n",
    "## How AdaBoost Works:\n",
    "\n",
    "1. Initialize equal weights for all training samples\n",
    "2. Train a weak learner on the weighted dataset\n",
    "3. Calculate the error rate and learner weight\n",
    "4. Increase weights of misclassified samples\n",
    "5. Repeat steps 2-4 for the specified number of estimators\n",
    "6. Combine all weak learners using weighted voting\n",
    "\n",
    "## Advantages:\n",
    "- Simple to implement\n",
    "- Works well with weak learners\n",
    "- Less prone to overfitting compared to other boosting methods\n",
    "- No need for extensive parameter tuning\n",
    "\n",
    "## Disadvantages:\n",
    "- Sensitive to noisy data and outliers\n",
    "- Can be slower to train than some other algorithms\n",
    "- Performance depends on the quality of weak learners"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251dac54",
   "metadata": {},
   "source": [
    "## Step 1: Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2473fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.datasets import make_classification\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21296ecc",
   "metadata": {},
   "source": [
    "## Step 2: Create a Synthetic Dataset\n",
    "\n",
    "We'll create a classification dataset for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b42d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a synthetic dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, \n",
    "                           n_redundant=5, random_state=42)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "print(f\"Training set size: {X_train.shape}\")\n",
    "print(f\"Testing set size: {X_test.shape}\")\n",
    "print(f\"Number of features: {X_train.shape[1]}\")\n",
    "print(f\"Class distribution in training set: {np.bincount(y_train)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7982b83b",
   "metadata": {},
   "source": [
    "## Step 3: Train AdaBoost Model\n",
    "\n",
    "We'll create an AdaBoost classifier with Decision Tree stumps as weak learners."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b46a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create base estimator (weak learner) - Decision Tree with max_depth=1 (stump)\n",
    "base_estimator = DecisionTreeClassifier(max_depth=1, random_state=42)\n",
    "\n",
    "# Create AdaBoost classifier\n",
    "ada_boost = AdaBoostClassifier(\n",
    "    estimator=base_estimator,\n",
    "    n_estimators=50,  # Number of weak learners\n",
    "    learning_rate=1.0,  # Weight applied to each classifier\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "ada_boost.fit(X_train, y_train)\n",
    "\n",
    "print(\"AdaBoost model trained successfully!\")\n",
    "print(f\"Number of estimators used: {ada_boost.n_estimators}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcadcbe1",
   "metadata": {},
   "source": [
    "## Step 4: Make Predictions and Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b559ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_train_pred = ada_boost.predict(X_train)\n",
    "y_test_pred = ada_boost.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Testing Accuracy: {test_accuracy:.4f}\")\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Classification Report (Test Set):\")\n",
    "print(\"=\"*50)\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6557bd5e",
   "metadata": {},
   "source": [
    "## Step 5: Visualize Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7999dc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create confusion matrix\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True)\n",
    "plt.title('Confusion Matrix - AdaBoost Classifier')\n",
    "plt.ylabel('Actual Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nConfusion Matrix:\\n{cm}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239a9bce",
   "metadata": {},
   "source": [
    "## Step 6: Analyze Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e540cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importances\n",
    "feature_importance = ada_boost.feature_importances_\n",
    "\n",
    "# Create a dataframe for better visualization\n",
    "feature_df = pd.DataFrame({\n",
    "    'Feature': [f'Feature_{i}' for i in range(len(feature_importance))],\n",
    "    'Importance': feature_importance\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "# Plot top 10 features\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(feature_df['Feature'][:10], feature_df['Importance'][:10])\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Importance')\n",
    "plt.title('Top 10 Feature Importances in AdaBoost')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Top 10 Most Important Features:\")\n",
    "print(feature_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67968b69",
   "metadata": {},
   "source": [
    "## Step 7: Effect of Number of Estimators\n",
    "\n",
    "Let's see how the number of estimators affects model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3837202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different numbers of estimators\n",
    "n_estimators_range = [1, 5, 10, 25, 50, 100, 150, 200]\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "\n",
    "for n_est in n_estimators_range:\n",
    "    ada = AdaBoostClassifier(\n",
    "        estimator=DecisionTreeClassifier(max_depth=1, random_state=42),\n",
    "        n_estimators=n_est,\n",
    "        learning_rate=1.0,\n",
    "        random_state=42\n",
    "    )\n",
    "    ada.fit(X_train, y_train)\n",
    "    \n",
    "    train_scores.append(ada.score(X_train, y_train))\n",
    "    test_scores.append(ada.score(X_test, y_test))\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(n_estimators_range, train_scores, marker='o', label='Training Accuracy')\n",
    "plt.plot(n_estimators_range, test_scores, marker='s', label='Testing Accuracy')\n",
    "plt.xlabel('Number of Estimators')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('AdaBoost Performance vs Number of Estimators')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(\"Performance by Number of Estimators:\")\n",
    "for n_est, train_acc, test_acc in zip(n_estimators_range, train_scores, test_scores):\n",
    "    print(f\"n_estimators={n_est:3d}: Train={train_acc:.4f}, Test={test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea5b599",
   "metadata": {},
   "source": [
    "## Key Parameters in AdaBoost:\n",
    "\n",
    "- **estimator**: The base weak learner (default: DecisionTreeClassifier with max_depth=1)\n",
    "- **n_estimators**: Number of weak learners to train sequentially\n",
    "- **learning_rate**: Weight applied to each classifier at each boosting iteration (lower values require more estimators)\n",
    "- **algorithm**: The boosting algorithm to use ('SAMME' or 'SAMME.R')\n",
    "\n",
    "## Conclusion:\n",
    "\n",
    "AdaBoost is a powerful ensemble method that:\n",
    "- Combines weak learners into a strong classifier\n",
    "- Focuses on difficult-to-classify instances\n",
    "- Often achieves high accuracy with simple base models\n",
    "- Works well when you have a good weak learner\n",
    "\n",
    "The key to AdaBoost's success is its adaptive nature - it learns from mistakes and focuses on the hardest cases!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
