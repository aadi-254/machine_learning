{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c0c16bd",
   "metadata": {},
   "source": [
    "# Random Forest Classifier\n",
    "\n",
    "## ðŸŒ² What is Random Forest?\n",
    "\n",
    "**Random Forest** is a powerful ensemble method built on **Bagging (Bootstrap Aggregating)** that:\n",
    "\n",
    "1. Trains multiple decision trees (forest of trees)\n",
    "2. On different random subsets of the training data (sampled with replacement)\n",
    "3. With random feature selection at each split (adds extra randomness)\n",
    "4. Combines their predictions (by voting for classification or averaging for regression)\n",
    "\n",
    "### ðŸ’¬ In short:\n",
    "\n",
    "> \"Train many decision trees on slightly different data with random features â†’ vote or average â†’ robust, accurate model.\"\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”„ How Random Forest Works (Example)\n",
    "\n",
    "Suppose we have a dataset with 100 samples and 4 features.\n",
    "\n",
    "1. **Random Forest** will randomly select bootstrapped samples â€” e.g. 80 random samples with replacement.\n",
    "2. It trains one decision tree on each sample, but at each split, only considers a random subset of features (e.g., 2 out of 4).\n",
    "3. Then it repeats that for 10, 20, 100, or more trees.\n",
    "\n",
    "**Final prediction:**\n",
    "- **Classification**: Majority vote across all trees\n",
    "- **Regression**: Average prediction across all trees\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“˜ How It Improves Accuracy\n",
    "\n",
    "- **Reduces variance** (overfitting) â€” Each tree sees different data and features\n",
    "- **Doesn't change bias** (average prediction) much\n",
    "- **Works well with unstable models** like decision trees\n",
    "- Each tree sees slightly different data + different features â†’ different errors â†’ averaging/voting cancels them out\n",
    "\n",
    "### Mathematically:\n",
    "\n",
    "$$\\text{Var}(\\bar{f}) = \\frac{1}{M^2} \\sum_{i=1}^{M} \\text{Var}(f_i) \\approx \\frac{1}{M} \\text{Var}(f)$$\n",
    "\n",
    "â†’ The more estimators ($M$), the lower the variance.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”§ Key Parameters\n",
    "\n",
    "| Parameter            | Description                                          |\n",
    "| -------------------- | ---------------------------------------------------- |\n",
    "| `n_estimators`       | Number of trees in the forest (default: 100)         |\n",
    "| `max_depth`          | Maximum depth of each tree (None = grow fully)       |\n",
    "| `max_features`       | Features considered at each split ('sqrt', 'log2')   |\n",
    "| `min_samples_split`  | Minimum samples required to split a node             |\n",
    "| `min_samples_leaf`   | Minimum samples required in a leaf node              |\n",
    "| `bootstrap`          | Sampling with replacement (True by default)          |\n",
    "| `oob_score`          | \"Out-of-bag\" score (built-in validation)             |\n",
    "| `n_jobs`             | Use multiple CPU cores (-1 = use all cores)          |\n",
    "| `random_state`       | Seed for reproducibility                             |\n",
    "| `criterion`          | Split quality measure ('gini', 'entropy')            |\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ†š Random Forest vs Bagging\n",
    "\n",
    "| Aspect              | Bagging                  | Random Forest               |\n",
    "| ------------------- | ------------------------ | --------------------------- |\n",
    "| Base Model          | Any model                | Always Decision Trees       |\n",
    "| Feature Selection   | Uses all features        | Random subset of features   |\n",
    "| Diversity           | From bootstrap samples   | Bootstrap + random features |\n",
    "| Typical Performance | Good                     | Better (more decorrelation) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bca1990",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90157ff7",
   "metadata": {},
   "source": [
    "## Step 1: Load Dataset and Split\n",
    "\n",
    "We'll use the Iris dataset to demonstrate Random Forest classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedcaea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_iris()\n",
    "x, y = data.data, data.target\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb57f83",
   "metadata": {},
   "source": [
    "## Step 2: Create, Train and Evaluate Random Forest\n",
    "\n",
    "We create a `RandomForestClassifier` with:\n",
    "- **100 trees**: More trees â†’ better performance (up to a point)\n",
    "- **max_depth=None**: Trees grow fully (may overfit individually, but ensemble handles it)\n",
    "- **oob_score=True**: Uses out-of-bag samples for validation (free cross-validation!)\n",
    "- **n_jobs=-1**: Uses all CPU cores for parallel training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3a128d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,        # number of trees\n",
    "    max_depth=None,          # let it grow fully\n",
    "    random_state=42,\n",
    "    oob_score=True,          # use out-of-bag validation\n",
    "    n_jobs=-1                # use all CPU cores\n",
    ")\n",
    "\n",
    "# Train model\n",
    "rf_model.fit(x_train, y_train)\n",
    "\n",
    "#predict\n",
    "y_pred = rf_model.predict(x_test)\n",
    "\n",
    "# Evaluate model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Random Forest Classifier Accuracy: {accuracy:.2f}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f0d809",
   "metadata": {},
   "source": [
    "## Step 3: Feature Importance Analysis\n",
    "\n",
    "Random Forest provides **feature importance scores** showing which features contribute most to predictions. This is calculated by measuring how much each feature decreases impurity across all trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6f4c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "importances = rf_model.feature_importances_\n",
    "features = load_iris().feature_names\n",
    "\n",
    "# Display importance\n",
    "df = pd.DataFrame({'Feature': features, 'Importance': importances})\n",
    "df = df.sort_values('Importance', ascending=False)\n",
    "print(df)\n",
    "\n",
    "# Plot\n",
    "plt.barh(df['Feature'], df['Importance'])\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Feature Importance in Random Forest')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126046d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
